{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8eb2e2f1-bc91-4a21-a134-e55f1f2a693c",
      "metadata": {
        "id": "8eb2e2f1-bc91-4a21-a134-e55f1f2a693c"
      },
      "source": [
        "## COMP0188 tutorial 1\n",
        "* This tutorial is designed to introduce Pytorch, training models with Pytorch and evaluating models using weights and biases. All of which will be critical for the rest of the course\n",
        "* Proficiency with Python is expected as well as a familiarity with object orientated programming within Python. For further information on Pytorch, please refer to https://pytorch.org/tutorials/beginner/basics/intro.html#learn-the-basics.\n",
        "* An introductory understanding to machine learning is also expected i.e., data set splitting, data loader, difference between sklearn and pytorch, feature selection.\n",
        "\n",
        "Connect environment to a GPU by:\n",
        "\n",
        "* Select 'Runtime' in the top left\n",
        "* Select 'Change Runtime Type'\n",
        "* Select the GPU runtime available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dcbce001-ff33-4463-ada8-41af92e9eb30",
      "metadata": {
        "id": "dcbce001-ff33-4463-ada8-41af92e9eb30"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from typing import Union, Callable, Tuple, List, Literal\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "random.seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8b41b99-29ee-4886-9084-8554ef8bd779",
      "metadata": {
        "id": "a8b41b99-29ee-4886-9084-8554ef8bd779"
      },
      "source": [
        "### Dataset\n",
        "* The diabetes dataset used in this tutorial is small and tabular therefore we'll use the standard dataloader and define a custom dataset to handle input data which:\n",
        "\n",
        "  * Is a pandas dataframe\n",
        "  * Has a 1-dimensional dependant variable which does not require processing\n",
        "  * Has an n-dimensional feature space which requires min-max scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "03adc6b2-cca0-4167-9496-55634e5b4dc7",
      "metadata": {
        "id": "03adc6b2-cca0-4167-9496-55634e5b4dc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(768, 9)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Pregnancies</th>\n",
              "      <th>Glucose</th>\n",
              "      <th>BloodPressure</th>\n",
              "      <th>SkinThickness</th>\n",
              "      <th>Insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>DiabetesPedigreeFunction</th>\n",
              "      <th>Age</th>\n",
              "      <th>Outcome</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
              "0            6      148             72             35        0  33.6   \n",
              "1            1       85             66             29        0  26.6   \n",
              "2            8      183             64              0        0  23.3   \n",
              "3            1       89             66             23       94  28.1   \n",
              "4            0      137             40             35      168  43.1   \n",
              "\n",
              "   DiabetesPedigreeFunction  Age  Outcome  \n",
              "0                     0.627   50        1  \n",
              "1                     0.351   31        0  \n",
              "2                     0.672   32        1  \n",
              "3                     0.167   21        0  \n",
              "4                     2.288   33        1  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load example dataset\n",
        "#file_dir = # Insert path to directory containing the file\n",
        "#df = pd.read_csv(os.path.join(file_dir, \"diabetes.csv\"))\n",
        "\n",
        "#script_dir = os.path.dirname(os.path.abspath(__file__))  # Directory of the current scriptï¼Œ However __file__ does not work in Jupyter Notebook\n",
        "#df = os.path.join(script_dir, '../data/diabetes.csv')  # Construct path\n",
        "\n",
        "#getcwd() method returns current working directory of a process, which works in Jupyter Notebook\n",
        "script_dir = os.getcwd()  # Current working directory\n",
        "file_path = os.path.join(script_dir, '../data/diabetes.csv')  # Construct path\n",
        "\n",
        "df = pd.read_csv(file_path)  # Load the dataset\n",
        "\n",
        "print(df.shape)\n",
        "y_var = \"Outcome\"\n",
        "X_vars = [col for col in df.columns if col != y_var]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9789fca-0ae0-4c1f-b032-f94fccfcd7d7",
      "metadata": {
        "id": "e9789fca-0ae0-4c1f-b032-f94fccfcd7d7"
      },
      "source": [
        "### Data Type and Visualization\n",
        "* Understanding your data is a critical step in any data science workflow. In this part, we analyze the data types and visualize the diabetes dataset to understand its structure and relationships between variables.\n",
        "  * Data Types Check: Checking data types helps ensure all features are in the correct format.\n",
        "  * Visualizing Distributions: Understanding the distribution of the target variable is essential for choosing the right model and evaluation metric.\n",
        "  * Pair Plot: A pair plot shows relationships between features and the target, allowing for a quick visual understanding of potential correlations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a449804c-dcc0-469c-b3c8-8c84b493dd41",
      "metadata": {
        "id": "a449804c-dcc0-469c-b3c8-8c84b493dd41"
      },
      "outputs": [],
      "source": [
        "# Check the data types of the features\n",
        "# Hint: a pandas DataFrame has an attribute \"dtype\"\n",
        "print() # YOUR CODE HERE\n",
        "\n",
        "# Visualizing the distribution of features to find target and independency features\n",
        "# Hint: Consider using the seaborn package\n",
        "# YOUR CODE HERE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f68bcc6-2108-4e65-9967-29356f592360",
      "metadata": {
        "id": "4f68bcc6-2108-4e65-9967-29356f592360"
      },
      "source": [
        "* Independent Variables: Usually numeric or categorical variables that represent different attributes or features.\n",
        "* Target Variable: Often numeric (for regression problems) or categorical (for classification problems). It is the variable that the model is trying to predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb8c2089-9338-4faf-8ef2-ce1f6c85b791",
      "metadata": {
        "id": "bb8c2089-9338-4faf-8ef2-ce1f6c85b791"
      },
      "outputs": [],
      "source": [
        "target_variable = 'Outcome'\n",
        "independent_variables = df.columns[df.columns != target_variable]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f768b92-d3c5-4ec4-a52e-2f2cdde38f28",
      "metadata": {
        "id": "2f768b92-d3c5-4ec4-a52e-2f2cdde38f28"
      },
      "source": [
        "### Train/Test Splits with Scikit-Learn\n",
        "* To build a reliable machine learning model, it is crucial to assess its performance on unseen data. To achieve this, we split the dataset into a training set, which is used to train the model, and a testing set, which is used to evaluate its performance. This helps in understanding the model's generalization ability.\n",
        "  * Utilize train_test_split from Scikit-Learn to split the data.\n",
        "  * Create training and testing datasets.\n",
        "  * Ensure that the test size is 20% (optional) of the original dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6181d0f3-b82b-4c11-9984-55add2a93f29",
      "metadata": {
        "id": "6181d0f3-b82b-4c11-9984-55add2a93f29",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "# YOUR CODE HERE\n",
        "\n",
        "print(f'Training data shape: {X_train.shape}, Testing data shape: {X_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5a179d5-4a87-4d82-8a77-3322436b16ab",
      "metadata": {
        "id": "c5a179d5-4a87-4d82-8a77-3322436b16ab"
      },
      "source": [
        "### Linear Regression with Scikit-Learn\n",
        "* Scikit-Learn LinearRegression provides a class to train a model to predict the target variable from the independent variables. The model will then be evaluated using the Mean Squared Error (MSE) metric.\n",
        "  * Linear Regression assumes that the relationship is linear, and it finds the line of best fit by minimizing the sum of squared differences between the observed and predicted values\n",
        "  * MSE is the average of the squared differences between the predicted and actual values. A lower MSE indicates a better fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96d10df2-d24d-4af9-912c-454d36c9a85d",
      "metadata": {
        "id": "96d10df2-d24d-4af9-912c-454d36c9a85d"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Initialize the linear regression model\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Train the model\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Predict on the test set\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Evaluate the model\n",
        "# YOUR CODE HERE\n",
        "print(f'Mean Squared Error on Test Set: {mse:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cb5a7f0-1699-4e72-92c1-82796a83cad6",
      "metadata": {
        "id": "8cb5a7f0-1699-4e72-92c1-82796a83cad6"
      },
      "source": [
        "### Pytorch Basic\n",
        "#### Tensor\n",
        "* Pytorch provides 'tensors' as the fundamental data structure which enable efficient linear algebra functionality, auto differentiation and integration with CUDA\n",
        "    * tens.T performs the transpose of the matrix\n",
        "    * Try pushing the tens to the GPU with tens.cuda()\n",
        "    * A torch.Tensor is a multi-dimensional matrix containing elements of a single data type.\n",
        "\n",
        "##### What's special about torch tensors?\n",
        "1. Torch tensors can be used on GPUs which are much faster than CPUs at large parallel computations\n",
        "2. Torch automatically keeps track of the gradient information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4450fc7-3545-46fa-87c2-2ac236a01121",
      "metadata": {
        "id": "f4450fc7-3545-46fa-87c2-2ac236a01121"
      },
      "outputs": [],
      "source": [
        "# Create multi-dimension tensor (4-dims)\n",
        "rands = torch.rand(1,2,3,4)\n",
        "print(rands)\n",
        "# torch.reshape returns a tensor with the same data and number of elements as input, but with the specified shape.\n",
        "print(rands.shape)\n",
        "\n",
        "# 1*2*3*4 = 1*(2*3)*4\n",
        "rands_reshape = rands.reshape(1, 6, 4)\n",
        "print(rands_reshape.shape)\n",
        "\n",
        "# A single dimension may be -1, in which case itâ€™s inferred from the remaining dimensions and the number of elements in input.\n",
        "rands_reshape = rands.reshape(1, -1)\n",
        "print(rands_reshape.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2f74dd2-0cc6-4401-b2eb-e8453e2e81a9",
      "metadata": {
        "id": "f2f74dd2-0cc6-4401-b2eb-e8453e2e81a9"
      },
      "outputs": [],
      "source": [
        "# Change our dataset to tensor\n",
        "tens = torch.tensor(df.values)\n",
        "print(tens)\n",
        "print(tens.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9cc351a7-2dd4-4d85-8096-8c002f5ae63f",
      "metadata": {
        "id": "9cc351a7-2dd4-4d85-8096-8c002f5ae63f"
      },
      "outputs": [],
      "source": [
        "# CPU vs GPU\n",
        "# Use GPU if available\n",
        "import time\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dim=6000\n",
        "\n",
        "x=torch.randn(dim,dim)\n",
        "y=torch.randn(dim,dim)\n",
        "start_time = time.time()\n",
        "z=torch.matmul(x,y)\n",
        "elapsed_time = time.time() - start_time\n",
        "print('CPU_time = ', elapsed_time)\n",
        "\n",
        "\n",
        "x=torch.randn(dim,dim,device=device)\n",
        "y=torch.randn(dim,dim,device=device)\n",
        "start_time = time.time()\n",
        "z=torch.matmul(x,y)\n",
        "elapsed_time = time.time() - start_time\n",
        "print('GPU_time = ',elapsed_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "679c0eaa-5215-4996-88cf-bc6cd656f40c",
      "metadata": {
        "id": "679c0eaa-5215-4996-88cf-bc6cd656f40c"
      },
      "outputs": [],
      "source": [
        "# Torch tensors automatically keep track of the gradient information\n",
        "a=torch.rand(64, requires_grad=True)\n",
        "\n",
        "b=4*a\n",
        "c=6*a\n",
        "\n",
        "out=(b+c).sum()\n",
        "out.backward()\n",
        "print(a.grad)\n",
        "\n",
        "# detach() can exclude some operations from gradient calculation, saving memory and computation\n",
        "a = a.detach()\n",
        "print(a.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "049227ac-a9fe-479c-9cf8-f070182aa762",
      "metadata": {
        "id": "049227ac-a9fe-479c-9cf8-f070182aa762"
      },
      "source": [
        "#### Pytorch Dataset and Dataloader\n",
        "Pytorch Datasets and Dataloaders provide a useful API for loading batches of data for deep learning models\n",
        "\n",
        "* Dataset\n",
        "  * The 'Dataset' represents the entire training/validation/test data. The \\_\\_len\\_\\_ and \\_\\_getitem\\_\\_ dunder methods are required for the Dataset class as they:\n",
        "    * Define the number of data observations e.g., a single row in a dataset, a single image and;\n",
        "    * Allow a single data observation to be retrieved\n",
        "    * The Dataset class simplifies managing large and non-standard datasets as e.g., not all of the data needs to be loaded into RAM at onces etc\n",
        "* DataLoader\n",
        "  * The 'Dataloader' handles how a given dataset should be batched. If the output of a dataset.\\_\\_getitem\\_\\_ call is a tensor then the base dataloader class can be used however, if non-standard types are being used i.e. dictionaries then defining custom batching is useful\n",
        "\n",
        "The diabetes dataset used in this tutorial is small and tabular therefore we'll use the standard dataloader and define a custom dataset to handle input data which:\n",
        "* Is a pandas dataframe;\n",
        "* Has a 1-dimensional dependant variable which does not require processing\n",
        "* Has an n-dimensional feature space which requires min-max scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baae5037-1200-46f9-ae61-8a3d8c137636",
      "metadata": {
        "id": "baae5037-1200-46f9-ae61-8a3d8c137636"
      },
      "outputs": [],
      "source": [
        "class PandasDataset(Dataset):\n",
        "    def __init__(self, X:pd.DataFrame, y:pd.Series)->None:\n",
        "        # Your code here\n",
        "        self._X = torch.from_numpy(X.values).float()\n",
        "        self._X = self.__min_max_norm(self._X)\n",
        "        self.feature_dim = X.shape[1]\n",
        "        self._len = X.shape[0]\n",
        "        self._y = torch.from_numpy(y.values)[:,None].float()\n",
        "\n",
        "    def __len__(self)->int:\n",
        "        # Your code here\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx:int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Your code here\n",
        "\n",
        "\n",
        "    def __min_max_norm(self, in_tens:torch.Tensor) -> torch.Tensor:\n",
        "        # Your code here\n",
        "        # Perform min-max normalization on the input tensor\n",
        "\n",
        "        # Calculate normalized tensor: (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
        "        # Note: Add a small epsilon to avoid division by zero\n",
        "        return norm_tens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e00c880-1484-4588-b439-af759fa205f6",
      "metadata": {
        "id": "6e00c880-1484-4588-b439-af759fa205f6"
      },
      "outputs": [],
      "source": [
        "# Split train_data to training and validation dataset\n",
        "# YOUR CODE HERE\n",
        "# Create datasets\n",
        "train_data = PandasDataset(X=X_train, y=y_train)\n",
        "val_data = PandasDataset(X=X_val, y=y_val)\n",
        "test_data = PandasDataset(X=X_test, y=y_test)\n",
        "# Get length of training, validation and test dataset\n",
        "print(f\"The training data has: {len(train_data)} samples\")\n",
        "print(f\"The validation data has: {len(val_data)} samples\")\n",
        "print(f\"The test data has: {len(test_data)} samples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fa549b4-90e9-45c6-b02e-10e28c747937",
      "metadata": {
        "id": "2fa549b4-90e9-45c6-b02e-10e28c747937"
      },
      "outputs": [],
      "source": [
        "# Let's load the dataset into PyTorch dataloaders, given the dataset is only small, a large batch size is not required.\n",
        "batch_size = 32\n",
        "shuffle = True\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
        "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=shuffle)\n",
        "print(f\"First train example: {train_data[0]} \\n with shape {(train_data[0][0].shape, train_data[0][1].shape)}\")\n",
        "print(\"\\n\")\n",
        "print(f\"Second train example: {train_data[1]} \\n with shape {(train_data[1][0].shape, train_data[1][1].shape)}\")\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5d4ea2c-f128-46a4-9fb1-37d79e2d6cee",
      "metadata": {
        "id": "f5d4ea2c-f128-46a4-9fb1-37d79e2d6cee"
      },
      "outputs": [],
      "source": [
        "# Notice how the dataloader concatenates the observations by adding a new first dimension\n",
        "first_batch = train_dataloader.__iter__()._next_data()\n",
        "print(f\"First train example: {first_batch} \\n with shape {(first_batch[0].shape, first_batch[1].shape)}\")\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5e9ccaa-1309-4523-9f31-6391d40d06a4",
      "metadata": {
        "id": "f5e9ccaa-1309-4523-9f31-6391d40d06a4"
      },
      "source": [
        "#### Pytorch Model\n",
        "* Pytorch models are developed by subclassing the nn.Module. The core requirement for a Pytorch model is defining the forward method which defines the model's forward pass. The new subclass will most likely make us of other nn.Module subclasses, some of which are:\n",
        "* nn.Linear(in_features, out_features) - this defines a single fully connected layer with a given number of input and output features\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0cb45ec",
      "metadata": {
        "id": "f0cb45ec"
      },
      "source": [
        "#### Linear Regression in PyTorch\n",
        "Unlike Scikit-Learn, PyTorch provides more flexibility for customizing the model architecture and training process. We define a simple linear model using nn.Module, specify the loss function, and use an optimizer to minimize the loss during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3d99b12-c587-4f71-a702-f9d79efc4943",
      "metadata": {
        "id": "b3d99b12-c587-4f71-a702-f9d79efc4943"
      },
      "outputs": [],
      "source": [
        "# Define the linear regression model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = 'cpu'\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def forward(self, x):\n",
        "        # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6343319e-4613-42d4-ba26-6cc5d82c2488",
      "metadata": {
        "id": "6343319e-4613-42d4-ba26-6cc5d82c2488"
      },
      "source": [
        "#### Training pipeline\n",
        "The train_single_epoch function provides an examplar function that trains an initialised model for a single epoch and returns the batch losses and predictions. Of note:\n",
        "* model.train(): certain nn.Module functionality such as dropout behaves differently during training and eval so we must tell the model that it is being trained\n",
        "* optimizer.zero_grad(), train_loss.backward() and optimizer.step(): for every minibatch, gradients are 'accumulated', based on this accumulation, the optimiser takes a 'step'. At the start of a gradient step the previous gradients are set to 0 to reaccumulate - _gradient calculations will be covered later in the course!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6972e365-0d23-4ee1-9f2f-953ff8db39fb",
      "metadata": {
        "id": "6972e365-0d23-4ee1-9f2f-953ff8db39fb"
      },
      "outputs": [],
      "source": [
        "def train_single_epoch(model, data_loader, criterion, optimizer):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for X_batch, y_batch in data_loader:\n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X_batch)\n",
        "        loss = criterion(y_pred, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(data_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d12c881-4274-4d0b-a499-20ff0715421d",
      "metadata": {
        "id": "1d12c881-4274-4d0b-a499-20ff0715421d"
      },
      "source": [
        "After each epoch, we would like to evaluate the model. Notice:\n",
        "* model.eval() now tells the model we are evaluating and ensures functionality such as dropout behave appropriately\n",
        "* torch.no_grad() tells the model not to calculate gradients since, in evaluation, we do not update the parameters!\n",
        "\n",
        "Complete the function to calculate the epoch lossses and predictions, take inspiraton from the training function above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd7b7535-f399-4dd4-aca9-b5ad37a6ad7c",
      "metadata": {
        "id": "bd7b7535-f399-4dd4-aca9-b5ad37a6ad7c"
      },
      "outputs": [],
      "source": [
        "def validate(model, data_loader, criterion):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            y_pred = model(X_batch)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            val_loss += loss.item()\n",
        "    return val_loss / len(data_loader)\n",
        "\n",
        "def test(model, data_loader, criterion):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in data_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            y_pred = model(X_batch)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            test_loss += loss.item()\n",
        "    return test_loss / len(data_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e18b98b-7c6d-4a9b-9f5a-72375ba0b431",
      "metadata": {
        "id": "0e18b98b-7c6d-4a9b-9f5a-72375ba0b431"
      },
      "source": [
        "* We can now use the above functions to run a single epoch worth of training and validation.\n",
        "* nn.MSELoss() is used since we are performing a linear regrassion task. This is not the only training metric which we can use again, experiment with others if you wish!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95c79be6-1f50-4332-a18e-382112deb26f",
      "metadata": {
        "id": "95c79be6-1f50-4332-a18e-382112deb26f"
      },
      "outputs": [],
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "input_dim = X_train.shape[1]\n",
        "model = LinearRegressionModel(input_dim)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train for one epoch and display the result\n",
        "train_loss_one_epoch = train_single_epoch(model, train_dataloader, criterion, optimizer)\n",
        "print(f'Training Loss after one epoch: {train_loss_one_epoch:.4f}')\n",
        "\n",
        "# Validate the model after one epoch\n",
        "val_loss_one_epoch = validate(model, val_dataloader, criterion)\n",
        "print(f'Validation Loss after one epoch: {val_loss_one_epoch:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78f9025b-37bc-4ccb-b90c-093280d2a2d4",
      "metadata": {
        "id": "78f9025b-37bc-4ccb-b90c-093280d2a2d4"
      },
      "source": [
        "#### Monitoring\n",
        "* A significant part of developing machine learning models involves experimentation. Tracking and managing these experiments can become challenging as the number of experiments grows. To address this, we use tools like Weights and Biases (wandb), which help in logging and visualizing metrics, saving model checkpoints, and comparing different runs effectively.\n",
        "* The training loop has been updated to log both training and validation metrics (such as loss) to wandb. This allows you to monitor the model's performance in real-time and keep track of the training process. Additionally, the model's parameters are saved at each epoch, but only the best-performing model (based on validation loss) is preserved. This ensures that you can always retrieve the best model from your experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d35a9330-999e-4025-9bc7-d2c75a6de5b3",
      "metadata": {
        "id": "d35a9330-999e-4025-9bc7-d2c75a6de5b3"
      },
      "outputs": [],
      "source": [
        "wandb.login()\n",
        "epochs = 50\n",
        "lr=0.001\n",
        "weight_decay=0.0\n",
        "\n",
        "config={\n",
        "    \"learning_rate\": lr,\n",
        "    \"architecture\": \"LinearRegressionModel\",\n",
        "    \"epochs\": epochs,\n",
        "    \"weight_decay\": weight_decay,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"shuffle\": shuffle,\n",
        "    \"loss\": criterion\n",
        "    }\n",
        "\n",
        "wandb.init(project='diabetes_prediction', config=config)\n",
        "\n",
        "\n",
        "def train_all_epochs(model, train_loader, val_loader, criterion, optimizer, epochs):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')  # Initialize best validation loss to a very high value\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Train for one epoch\n",
        "        model.train()  # Set model to training mode\n",
        "        train_loss = train_single_epoch(model, train_loader, criterion, optimizer)\n",
        "\n",
        "        # Validate the model after the epoch\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        with torch.no_grad():  # Disable gradient calculation for validation\n",
        "            val_loss = validate(model, val_loader, criterion)\n",
        "\n",
        "        # Log the training and validation loss to wandb\n",
        "        wandb.log({\"train_loss\": train_loss, \"val_loss\": val_loss})\n",
        "\n",
        "        # Save the best model checkpoint\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            chkp_pth = os.path.join(wandb.run.dir, f\"mdl_chkpnt_epoch_{epoch}.pt\")\n",
        "            torch.save(\n",
        "                {\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                }, chkp_pth)\n",
        "            # Log the path of the best model checkpoint\n",
        "            wandb.log({\"best_model_path\": chkp_pth})  # Optionally log the path\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "    return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab5d2d3c-6494-4cbf-a647-0fd3e72f3198",
      "metadata": {
        "id": "ab5d2d3c-6494-4cbf-a647-0fd3e72f3198"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "train_losses, val_losses = train_all_epochs(model, train_dataloader, val_dataloader, criterion, optimizer, epochs)\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7f460e7-475e-4d70-82af-8ab2b5f50e33",
      "metadata": {
        "id": "a7f460e7-475e-4d70-82af-8ab2b5f50e33"
      },
      "outputs": [],
      "source": [
        "# Plot MSE loss over epochs\n",
        "# Hint: Use the matplotlib package and the plot object\n",
        "# Multiple lines can be placed on a single graph by calling plot multiple times before calling show\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b210aca-6488-4c0e-b94f-323a6f99446c",
      "metadata": {
        "id": "2b210aca-6488-4c0e-b94f-323a6f99446c"
      },
      "source": [
        "### Feature Selection\n",
        "Feature selection is a technique to improve model performance by focusing on the most relevant features. We hypothesize which features are most important and iteratively add features to our model to observe changes in performance. This can help in building more efficient models.\n",
        "* The f_regression method in the sklearn package helps evaluate the features' significance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb9c2227-07e5-4b00-b15f-2a092cb78184",
      "metadata": {
        "id": "cb9c2227-07e5-4b00-b15f-2a092cb78184"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import f_regression\n",
        "\n",
        "# Start with the most important feature\n",
        "initial_feature = 'Glucose'  # Hypothetical important feature\n",
        "\n",
        "# Create a new dataset with only the selected features\n",
        "selected_features = [initial_feature]\n",
        "\n",
        "# Loop to add one feature at a time and check performance\n",
        "for feature in independent_variables:\n",
        "    if feature not in selected_features:\n",
        "        selected_features.append(feature)\n",
        "        X_train_subset = X_train[selected_features]\n",
        "        X_test_subset = X_test[selected_features]\n",
        "\n",
        "        # Train a new model with the selected features\n",
        "        model = LinearRegression()\n",
        "        model.fit(X_train_subset, y_train)\n",
        "        y_pred = model.predict(X_test_subset)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "        print(f'Features: {selected_features}, MSE: {mse:.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "775ff520",
      "metadata": {
        "id": "775ff520"
      },
      "source": [
        "### Extended exercise 1\n",
        "* Update the Dataset class and train functions to make running the model on a GPU more efficient! _Hint: Front load the data being pushed!_\n",
        "* Compare differences in time consumption"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30d71cbf",
      "metadata": {
        "id": "30d71cbf"
      },
      "source": [
        "### Extended exercise 2\n",
        "* Using weights and biases to diagnose model performance, try and develop the best performing model\n",
        "* Don't evaluate the model on the test set until you are finished with experimentation"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
